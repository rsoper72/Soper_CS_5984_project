{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dialogs from Cornell Movie Corpus, Analyze, Condition, Store\n",
    "https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
    "\n",
    "Retain:\n",
    "- lines with >=5 and <=60 words\n",
    "- words with occurance >= 210 times in all dialog remaining dialog lines\n",
    "- lines with < 20% occurance of rare words (occurance < 210 times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conversations txt file associate line numbers as sequential dialog -- remove unnecessary metadata\n",
    "F_c = open('data\\cornell movie-dialogs corpus\\movie_conversations.txt')\n",
    "F_cw = open('data\\cornell movie-dialogs corpus\\movie_conversations2.txt','w')\n",
    "for row in F_c:\n",
    "    cur = row.split(r' +++$+++ ')[3].rstrip()[1:-1]\n",
    "    F_cw.write(cur+'\\n')\n",
    "F_cw.close()\n",
    "F_c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct dictionary of \"clean\" movie dialog lines keyed to line number\n",
    "D_lines = {}\n",
    "F_l = open('data\\cornell movie-dialogs corpus\\movie_lines.txt')\n",
    "for row in F_l:\n",
    "    loc = row.find(r' +++$+++ ')\n",
    "    loc2 = row.rfind(r' +++$+++ ')\n",
    "    line_ind = row[:loc]\n",
    "    mline = row[loc2+9:].replace(\"'\", \" '\")\n",
    "    mline = re.sub(r'((?:[^A-Za-z\\s]|\\s)+)', lambda x: ' ' if ' ' in x.group(0) else '', mline.rstrip().lower())\n",
    "    mline = re.sub(' +', ' ', mline)\n",
    "    D_lines[line_ind] = mline\n",
    "F_l.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = 'data/D_lines.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(D_lines,file_obj)   \n",
    "file_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304713"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 304,713 total lines available (not the same as paired lines in dialog)\n",
    "len(D_lines.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "563"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maximum number of words in a line = 563 ... too many\n",
    "max_words = 0\n",
    "for line in D_lines.values():\n",
    "    cur = len(line.split(' '))\n",
    "    if cur > max_words:\n",
    "        max_words = cur\n",
    "max_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211628"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restrict number of words in dialog lines to what *might* be reasonable >=5 words, <= 60 words ... leaves 211,628 lines\n",
    "D_midlines = {}\n",
    "for k in D_lines:\n",
    "    if len(D_lines[k].split(' ')) <= 60 and len(D_lines[k].split(' ')) >= 5:\n",
    "        D_midlines[k] = D_lines[k]\n",
    "len(D_midlines.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'L194', 'L195', 'L196', 'L197'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'L198', 'L199'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'L200', 'L201', 'L202', 'L203'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'L204', 'L205', 'L206'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'L207', 'L208'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0\n",
       "0  'L194', 'L195', 'L196', 'L197'\n",
       "1                  'L198', 'L199'\n",
       "2  'L200', 'L201', 'L202', 'L203'\n",
       "3          'L204', 'L205', 'L206'\n",
       "4                  'L207', 'L208'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = pd.read_table('data\\cornell movie-dialogs corpus\\movie_conversations2.txt', header=None)\n",
    "conv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['L194', 'L195', 'L196', 'L197'],\n",
       " ['L198', 'L199'],\n",
       " ['L200', 'L201', 'L202', 'L203'],\n",
       " ['L204', 'L205', 'L206'],\n",
       " ['L207', 'L208'],\n",
       " ['L271', 'L272', 'L273', 'L274', 'L275'],\n",
       " ['L276', 'L277'],\n",
       " ['L280', 'L281'],\n",
       " ['L363', 'L364'],\n",
       " ['L365', 'L366']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2 = []\n",
    "for row in conv[0]:\n",
    "    l_s = row.split(',')\n",
    "    row2 = []\n",
    "    for L in l_s:\n",
    "        row2.append(L[L.find(\"'\")+1:L.rfind(\"'\")])\n",
    "    conv2.append(row2)\n",
    "conv2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107662\n",
      "forth:  L194 , back:  L195\n",
      "forth:  L195 , back:  L196\n",
      "forth:  L196 , back:  L197\n",
      "forth:  L202 , back:  L203\n",
      "forth:  L207 , back:  L208\n",
      "forth:  L271 , back:  L272\n",
      "forth:  L272 , back:  L273\n",
      "forth:  L273 , back:  L274\n",
      "forth:  L276 , back:  L277\n",
      "forth:  L363 , back:  L364\n"
     ]
    }
   ],
   "source": [
    "# map dialog sequences to back-and-forth individual initiating sequence/response sequence pairs\n",
    "forth = []\n",
    "back = []\n",
    "for row in conv2:\n",
    "    for line in range(len(row)-1):\n",
    "        if (row[line] in D_midlines) and (row[line+1] in D_midlines):\n",
    "            forth.append(row[line])\n",
    "            back.append(row[line+1])\n",
    "print(len(forth))\n",
    "for i in range(10):\n",
    "    print('forth: ', forth[i], ', back: ', back[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56174"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct set of all unique words -- 56,174 words in lines of 5 to 60 words\n",
    "word_list = set()\n",
    "for line in D_midlines.values():\n",
    "    for word in line.split(' '):\n",
    "        word_list.add(word)\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used 13 times or less =  55193 ,  98.25364047424075 % of all words\n",
      "Total words =  56174 , words used 14+ times =  981 ,  1.7463595257592481 %\n",
      "10 most common\n",
      "['you', 128756]\n",
      "['i', 121549]\n",
      "['the', 86658]\n",
      "['to', 72910]\n",
      "['a', 62274]\n",
      "['s', 57012]\n",
      "['it', 56315]\n",
      "['t', 49626]\n",
      "['that', 39180]\n",
      "['and', 38207]\n"
     ]
    }
   ],
   "source": [
    "# dictionary counting instances of [word] in all dialog lines\n",
    "word_count = dict()\n",
    "for word in word_list:\n",
    "    word_count[word] = 0\n",
    "for line in D_midlines.values():\n",
    "    for word in line.split(' '):\n",
    "        word_count[word] += 1\n",
    "word_ordered = []\n",
    "for word in word_count:\n",
    "    word_ordered.append([word, word_count[word]])\n",
    "word_ordered.sort(key = lambda x : x[1])\n",
    "count_thrt_m = 0\n",
    "i = 0\n",
    "while True:\n",
    "    if word_ordered[i][1] < 210:  # define \"rare\" words as those occuring <14 in all lines\n",
    "        count_thrt_m += 1\n",
    "        i += 1\n",
    "    else:\n",
    "        break\n",
    "print('Used 13 times or less = ', count_thrt_m, ', ', count_thrt_m/len(word_list)*100, '% of all words')\n",
    "print('Total words = ', len(word_list), ', words used 14+ times = ', \n",
    "      len(word_list)-count_thrt_m, ', ',\n",
    "      (len(word_list)-count_thrt_m)/len(word_list)*100,'%')\n",
    "print('10 most common')\n",
    "for i in range(10):\n",
    "    print(word_ordered[-1-i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with 15%+ rare word (<14 uses) content  53769 ,  25.407318502277583 % of lines\n"
     ]
    }
   ],
   "source": [
    "# include lines that have high (>20% occurance) of rare words in discard set\n",
    "high_rare = set()\n",
    "for (line_n, line) in D_midlines.items():\n",
    "    line_s = line.split()\n",
    "    num_w = len(line_s)\n",
    "    count_r = 0\n",
    "    for word in line_s:\n",
    "        if word_count[word] < 210:\n",
    "            count_r += 1\n",
    "    if count_r/num_w > 0.20:\n",
    "            high_rare.add(line_n)\n",
    "print('Lines with 15%+ rare word (<14 uses) content ', len(high_rare),', ', len(high_rare)/len(D_midlines)*100, '% of lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num conversations (non-rare):  63689\n",
      "Num words in intiating exchange:  844080\n",
      "Num unique words in intiating exchange:  981\n",
      "Example initiating exchange:\n",
      "[['right', 'see', 'you', 're', 'ready', 'for', 'the', 'UNK'], ['i', 'don', 't', 'want', 'to', 'know', 'how', 'to', 'say', 'that', 'though', 'i', 'want', 'to', 'know', 'UNK', 'things', 'like', 'where', 'the', 'good', 'UNK', 'are', 'how', 'much', 'does', 'UNK', 'cost', 'stuff', 'like', 'UNK', 'i', 'have', 'never', 'in', 'my', 'life', 'had', 'to', 'point', 'out', 'my', 'head', 'to', 'someone'], ['how', 'is', 'our', 'little', 'find', 'the', 'UNK', 'a', 'date', 'plan', 'UNK'], ['you', 'got', 'something', 'on', 'your', 'mind'], ['i', 'really', 'really', 'really', 'wanna', 'go', 'but', 'i', 'can', 't', 'not', 'unless', 'my', 'sister', 'goes'], ['so', 'that', 's', 'the', 'kind', 'of', 'guy', 'she', 'likes', 'pretty', 'ones'], ['sometimes', 'i', 'wonder', 'if', 'the', 'guys', 'we', 're', 'supposed', 'to', 'want', 'to', 'go', 'out', 'with', 'are', 'the', 'ones', 'we', 'actually', 'want', 'to', 'go', 'out', 'with', 'you', 'know'], ['i', 'have', 'to', 'be', 'home', 'in', 'twenty', 'minutes'], ['listen', 'i', 'want', 'to', 'talk', 'to', 'you', 'about', 'the', 'UNK'], ['i', 'have', 'the', 'UNK', 'to', 'UNK', 'the', 'UNK', 'out', 'of', 'you', 'if', 'you', 'don', 't', 'get', 'out', 'of', 'my', 'way']]\n",
      "\n",
      "Num words in response:  869397\n",
      "Num unique words in response:  981\n",
      "Example response:\n",
      "[['i', 'don', 't', 'want', 'to', 'know', 'how', 'to', 'say', 'that', 'though', 'i', 'want', 'to', 'know', 'UNK', 'things', 'like', 'where', 'the', 'good', 'UNK', 'are', 'how', 'much', 'does', 'UNK', 'cost', 'stuff', 'like', 'UNK', 'i', 'have', 'never', 'in', 'my', 'life', 'had', 'to', 'point', 'out', 'my', 'head', 'to', 'someone'], ['that', 's', 'because', 'it', 's', 'such', 'a', 'nice', 'one'], ['well', 'there', 's', 'someone', 'i', 'think', 'might', 'be'], ['i', 'UNK', 'on', 'you', 'to', 'help', 'my', 'cause', 'you', 'and', 'that', 'UNK', 'are', 'UNK', 'UNK', 'aren', 't', 'we', 'ever', 'going', 'on', 'our', 'date'], ['i', 'm', 'UNK', 'on', 'it', 'but', 'she', 'doesn', 't', 'seem', 'to', 'be', 'goin', 'for', 'him'], ['who', 'knows', 'all', 'i', 've', 'ever', 'heard', 'her', 'say', 'is', 'that', 'she', 'd', 'UNK', 'before', 'UNK', 'a', 'guy', 'that', 'UNK'], ['all', 'i', 'know', 'is', 'i', 'd', 'give', 'up', 'my', 'private', 'line', 'to', 'go', 'out', 'with', 'a', 'guy', 'like', 'UNK'], ['i', 'don', 't', 'have', 'to', 'be', 'home', 'UNK', 'two'], ['you', 'know', 'the', 'deal', 'i', 'can', 't', 'go', 'if', 'UNK', 'doesn', 't', 'go'], ['can', 'you', 'at', 'least', 'start', 'wearing', 'a', 'UNK']]\n",
      "\n",
      "Total unique words -- initiations and responses in exchanges 981\n"
     ]
    }
   ],
   "source": [
    "# construct conditioned dialog sets -- x = initiating sequence, y = response sequence\n",
    "x_train_test = []\n",
    "y_train_test = []\n",
    "x_vec = []\n",
    "y_vec = []\n",
    "num_wrds_in = 0\n",
    "num_wrds_out = 0\n",
    "wrd_set = set()\n",
    "wrd_set_in = set()\n",
    "wrd_set_out = set()\n",
    "forth_nr = []\n",
    "back_nr = []\n",
    "t = 0\n",
    "for i in range(len(forth)):\n",
    "    if (forth[i] in high_rare) or (back[i] in high_rare):\n",
    "        continue\n",
    "    else:\n",
    "        forth_nr.append(forth[i])\n",
    "        back_nr.append(back[i])\n",
    "        cur1 = D_midlines[forth[i]].split()\n",
    "        for (k,wrd) in enumerate(cur1):\n",
    "            if word_count[wrd] < 210:\n",
    "                cur1[k] = 'UNK'\n",
    "        x_train_test.append(cur1)\n",
    "        num_wrds_in += len(cur1)\n",
    "        for wrd in cur1:\n",
    "            wrd_set.add(wrd)\n",
    "            wrd_set_in.add(wrd)\n",
    "        cur2 = D_midlines[back[i]].split()\n",
    "        for (k,wrd) in enumerate(cur2):\n",
    "            if word_count[wrd] < 210:\n",
    "                cur2[k] = 'UNK'\n",
    "        y_train_test.append(cur2)\n",
    "        num_wrds_out += len(cur2)\n",
    "        for wrd in cur2:\n",
    "            wrd_set.add(wrd)\n",
    "            wrd_set_out.add(wrd)\n",
    "        t += 1\n",
    "print('Num conversations (non-rare): ', len(x_train_test))\n",
    "print('Num words in intiating exchange: ', num_wrds_in)\n",
    "print('Num unique words in intiating exchange: ', len(wrd_set_in))\n",
    "print('Example initiating exchange:')\n",
    "print(x_train_test[:10])\n",
    "print()\n",
    "print('Num words in response: ', num_wrds_out)\n",
    "print('Num unique words in response: ', len(wrd_set_out))\n",
    "print('Example response:')\n",
    "print(y_train_test[:10])\n",
    "print()\n",
    "print('Total unique words -- initiations and responses in exchanges', len(wrd_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = 'data/x_train_test5.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(x_train_test,file_obj)   \n",
    "file_obj.close()\n",
    "file_name = 'data/y_train_test5.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(y_train_test,file_obj)   \n",
    "file_obj.close()\n",
    "file_name = 'data/forth_nr5.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(forth_nr,file_obj)   \n",
    "file_obj.close()\n",
    "file_name = 'data/back_nr5.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(back_nr,file_obj)   \n",
    "file_obj.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
