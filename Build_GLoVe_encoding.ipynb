{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Words as 50 Dimension Vectors Based on Stanford NLP GLoVe Pre-Trained Wikipedia 2014 + GigaWord 5\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load conditioned movie dialogs\n",
    "file_name = 'data/x_train_test4.pkl'\n",
    "file_obj = open(file_name,'rb') \n",
    "x_words = pickle.load(file_obj)   \n",
    "file_obj.close()\n",
    "file_name = 'data/y_train_test4.pkl'\n",
    "file_obj = open(file_name,'rb') \n",
    "y_words = pickle.load(file_obj)   \n",
    "file_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# colapse all to lower-case\n",
    "for i in range(len(x_words)):\n",
    "    x_words[i] = list(map(lambda x: x.lower(), x_words[i]))\n",
    "    y_words[i] = list(map(lambda x: x.lower(), y_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91458"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 91,458 total dialogs in available corpus\n",
    "len(x_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete GLoVe dictionary\n",
    "f = open('data/glove_6B_50d.txt',encoding='utf-8')\n",
    "wrd_vec_all = {}\n",
    "for row in f:\n",
    "    row_vec = row.split()\n",
    "    for j in range(1,len(row_vec)):\n",
    "        row_vec[j] = float(row_vec[j])\n",
    "    wrd_vec_all[row_vec[0]] = row_vec[1:]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset -- dictionary of local corpus intersection\n",
    "wrd_vec = dict()\n",
    "for i in range(len(x_words)):\n",
    "    for (j,word) in enumerate(x_words[i]):\n",
    "        if word in wrd_vec_all:\n",
    "            wrd_vec[word] = wrd_vec_all[word]\n",
    "        else:\n",
    "            x_words[i][j] = 'unk'\n",
    "for i in range(len(y_words)):\n",
    "    for (j,word) in enumerate(y_words[i]):\n",
    "        if word in wrd_vec_all:\n",
    "            wrd_vec[word] = wrd_vec_all[word]\n",
    "        else:\n",
    "            y_words[i][j] = 'unk'\n",
    "wrd_vec['unk'] = wrd_vec_all['_____']  # define vector for low-frequency and not-found words\n",
    "wrd_vec['*start*'] = wrd_vec_all['*']  # unused icon as start word for decoder input\n",
    "wrd_vec['*stop*'] = wrd_vec_all['^']  # unused icon as stop word for decoder target/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomly split into training and test sets  -- 60,000 dialogs for training; 20,000 for test\n",
    "import random\n",
    "random.seed(32)\n",
    "random.shuffle(x_words)\n",
    "random.seed(32)\n",
    "random.shuffle(y_words)\n",
    "x_train = x_words[:60000]\n",
    "x_test = x_words[60000:80000]\n",
    "y_train = y_words[:60000]\n",
    "y_test = y_words[60000:80000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode training vectors\n",
    "zer_vec = [0]*50\n",
    "x_tr_vec = []\n",
    "y_tr_in_vec = []\n",
    "y_tr_tar_vec = []\n",
    "x_tr_r_vec = []\n",
    "for i in range(len(x_train)):\n",
    "    for j in range(60):\n",
    "        if j == 0:\n",
    "            x_tr_vec.append([wrd_vec[x_train[i][j]]])\n",
    "        elif j < len(x_train[i]):\n",
    "            x_tr_vec[i].append(wrd_vec[x_train[i][j]])\n",
    "        else:\n",
    "            x_tr_vec[i].append(zer_vec[:])\n",
    "    x_tr_r_vec.append([x_tr_vec[i][-1]])\n",
    "    for j in range(1,60):\n",
    "        x_tr_r_vec[i].append(x_tr_vec[i][-1-j])\n",
    "for i in range(len(y_train)):\n",
    "    for j in range(len(y_train[i])+1):\n",
    "        if j == 0:\n",
    "            y_tr_in_vec.append([wrd_vec['*start*']])\n",
    "            y_tr_tar_vec.append([wrd_vec[y_train[i][j]]])\n",
    "        else:\n",
    "            y_tr_in_vec[i].append(wrd_vec[y_train[i][j-1]])\n",
    "            if j == len(y_train[i]):\n",
    "                y_tr_tar_vec[i].append(wrd_vec['*stop*'])\n",
    "            else:\n",
    "                y_tr_tar_vec[i].append(wrd_vec[y_train[i][j]])\n",
    "for i in range(len(y_train)):\n",
    "    for j in range(len(y_train[i]),60):\n",
    "        y_tr_in_vec[i].append(zer_vec[:])\n",
    "        y_tr_tar_vec[i].append(zer_vec[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save training vectors\n",
    "file_name = 'data/x_tr_vec4.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(x_tr_vec, file_obj)   \n",
    "file_obj.close()\n",
    "file_name = 'data/x_tr_r_vec4.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(x_tr_r_vec, file_obj)   \n",
    "file_obj.close()\n",
    "file_name = 'data/y_tr_in_vec4.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(y_tr_in_vec, file_obj)   \n",
    "file_obj.close()\n",
    "file_name = 'data/y_tr_tar_vec4.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(y_tr_tar_vec, file_obj)   \n",
    "file_obj.close()\n",
    "file_name = 'data/wrd_vec4.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(wrd_vec, file_obj)   \n",
    "file_obj.close()\n",
    "\n",
    "del x_tr_vec, x_tr_r_vec, y_tr_in_vec, y_tr_tar_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode test vectors\n",
    "zer_vec = [0]*50\n",
    "x_ts_vec = []\n",
    "y_ts_in_vec = []\n",
    "y_ts_tar_vec = []\n",
    "x_ts_r_vec = []\n",
    "for i in range(len(x_test)):\n",
    "    for j in range(60):\n",
    "        if j == 0:\n",
    "            x_ts_vec.append([wrd_vec[x_test[i][j]]])\n",
    "        elif j < len(x_test[i]):\n",
    "            x_ts_vec[i].append(wrd_vec[x_test[i][j]])\n",
    "        else:\n",
    "            x_ts_vec[i].append(zer_vec[:])\n",
    "    x_ts_r_vec.append([x_ts_vec[i][-1]])\n",
    "    for j in range(1,60):\n",
    "        x_ts_r_vec[i].append(x_ts_vec[i][-1-j])\n",
    "for i in range(len(y_test)):\n",
    "    for j in range(len(y_test[i])+1):\n",
    "        if j == 0:\n",
    "            y_ts_in_vec.append([wrd_vec['*start*']])\n",
    "            y_ts_tar_vec.append([wrd_vec[y_test[i][j]]])\n",
    "        else:\n",
    "            y_ts_in_vec[i].append(wrd_vec[y_test[i][j-1]])\n",
    "            if j == len(y_test[i]):\n",
    "                y_ts_tar_vec[i].append(wrd_vec['*stop*'])\n",
    "            else:\n",
    "                y_ts_tar_vec[i].append(wrd_vec[y_test[i][j]])\n",
    "for i in range(len(y_test)):\n",
    "    for j in range(len(y_test[i]),60):\n",
    "        y_ts_in_vec[i].append(zer_vec[:])\n",
    "        y_ts_tar_vec[i].append(zer_vec[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save test vectors\n",
    "file_name = 'data/x_ts_vec4.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(x_ts_vec, file_obj)   \n",
    "file_obj.close()\n",
    "file_name = 'data/x_ts_r_vec4.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(x_ts_r_vec, file_obj)   \n",
    "file_obj.close()\n",
    "file_name = 'data/y_ts_in_vec4.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(y_ts_in_vec, file_obj)   \n",
    "file_obj.close()\n",
    "file_name = 'data/y_ts_tar_vec4.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(y_ts_tar_vec, file_obj)   \n",
    "file_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct training and test vectors for seq2seq bot detector (40,000 training, 40,000 test)\n",
    "import random\n",
    "random.seed(48)\n",
    "random.shuffle(x_words)\n",
    "random.seed(48)\n",
    "random.shuffle(y_words)\n",
    "x = x_words[:40000]\n",
    "y = y_words[:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zer_vec = [0]*50\n",
    "x_tur_vec = []\n",
    "y_tur_in_vec = []\n",
    "y_hum_tar_vec = []\n",
    "x_tur_r_vec = []\n",
    "for i in range(len(x)):\n",
    "    for j in range(60):\n",
    "        if j == 0:\n",
    "            x_tur_vec.append([wrd_vec[x[i][j]]])\n",
    "        elif j < len(x[i]):\n",
    "            x_tur_vec[i].append(wrd_vec[x[i][j]])\n",
    "        else:\n",
    "            x_tur_vec[i].append(zer_vec[:])\n",
    "    x_tur_r_vec.append([x_tur_vec[i][-1]])\n",
    "    for j in range(1,60):\n",
    "        x_tur_r_vec[i].append(x_tur_vec[i][-1-j])\n",
    "for i in range(len(y)):\n",
    "    for j in range(len(y[i])+1):\n",
    "        if j == 0:\n",
    "            y_tur_in_vec.append([wrd_vec['*start*']])\n",
    "            y_hum_tar_vec.append([wrd_vec[y[i][j]]])\n",
    "        else:\n",
    "            y_tur_in_vec[i].append(wrd_vec[y[i][j-1]])\n",
    "            if j == len(y[i]):\n",
    "                y_hum_tar_vec[i].append(wrd_vec['*stop*'])\n",
    "            else:\n",
    "                y_hum_tar_vec[i].append(wrd_vec[y[i][j]])\n",
    "for i in range(len(y)):\n",
    "    for j in range(len(y[i]),60):\n",
    "        y_tur_in_vec[i].append(zer_vec[:])\n",
    "        y_hum_tar_vec[i].append(zer_vec[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = 'data/x_tur_vec4.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(x_tur_vec, file_obj)   \n",
    "file_obj.close()\n",
    "file_name = 'data/x_tur_r_vec4.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(x_tur_r_vec, file_obj)   \n",
    "file_obj.close()\n",
    "file_name = 'data/y_tur_in_vec4.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(y_tur_in_vec, file_obj)   \n",
    "file_obj.close()\n",
    "file_name = 'data/y_hum_tar_vec4.pkl'\n",
    "file_obj = open(file_name,'wb') \n",
    "pickle.dump(y_hum_tar_vec, file_obj)   \n",
    "file_obj.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
